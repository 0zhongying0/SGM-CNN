{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DAE(object):\n",
    "\n",
    "\t\"\"\"\n",
    "\tDenoising autoencoder. Gaussian noise is added. The scale and standard deviation\n",
    "\tof it are noise_scale and noise_std, respectively.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, n_feature, n_hidden, noise_scale, noise_std, reg_lamda = 0.01):\n",
    "\n",
    "\t\tself.n_hidden = n_hidden\n",
    "\t\tself.n_feature = n_feature\n",
    "\t\tself.reg_lamda = reg_lamda\n",
    "\t\tself.noise_scale = noise_scale\n",
    "\t\tself.noise_std = noise_std\n",
    "\n",
    "\t\tself.data = tf.placeholder(shape = [None, n_feature],\n",
    "\t\t                           dtype = tf.float64)\n",
    "\t\tself.noise = self.noise_scale * tf.random_normal([n_feature], dtype = tf.float64,\n",
    "\t\t                                               stddev = self.noise_std)\n",
    "\t\tdata_with_noise = self.data + self.noise\n",
    "\n",
    "\t\tself.weight_encoder = tf.get_variable(name = 'weight_encoder', \n",
    "\t\t\t                    shape = [self.n_feature, self.n_hidden],\n",
    "\t\t\t                    dtype = tf.float64)\n",
    "\t\tself.bias_encoder = tf.Variable(tf.zeros([self.n_hidden], \n",
    "\t\t\t                                      dtype = tf.float64),\n",
    "\t\t                                name = 'bias_encoder')\n",
    "\n",
    "\t\tweight_decoder = tf.get_variable(name = 'weight_decoder', \n",
    "\t\t\t                    shape = [self.n_hidden, self.n_feature],\n",
    "\t\t\t                    dtype = tf.float64)\n",
    "\t\tbias_decoder = tf.Variable(tf.zeros([self.n_feature], dtype = tf.float64),\n",
    "\t\t\t                           name = 'bias_decoder')\n",
    "\n",
    "\t\twith tf.name_scope('Encoder'):\n",
    "\t\t\tdata_encoded = tf.add(tf.matmul(data_with_noise, self.weight_encoder),\n",
    "\t\t\t\t                  self.bias_encoder)\n",
    "\t\t\tdata_encoded = tf.nn.tanh(data_encoded)\n",
    "\n",
    "\t\twith tf.name_scope('Decoder'):\n",
    "\t\t\tdata_recons = tf.add(tf.matmul(data_encoded, weight_decoder),\n",
    "\t\t\t\t                 bias_decoder)\n",
    "\t\t\tself.data_recons = tf.tanh(data_recons)\n",
    "\n",
    "\t\twith tf.name_scope('Loss'):\n",
    "\t\t\tdiff = self.data_recons - self.data\n",
    "\t\t\tself.loss_mse = 0.5 * tf.reduce_mean(tf.reduce_sum(diff**2, axis = 1))\n",
    "\t\t\tloss_reg = tf.reduce_sum(tf.sqrt(tf.reduce_sum(self.weight_encoder ** 2, axis = 1))) \n",
    "\t\t\tself.loss_reg = self.reg_lamda * loss_reg\n",
    "\t\t\tself.l2_loss = tf.nn.l2_loss(weight_decoder) * 1E-3\n",
    "\n",
    "\t\t\tself.loss = self.loss_mse + self.loss_reg + self.l2_loss\n",
    "\n",
    "\t\twith tf.name_scope('weight_vector'):\n",
    "\t\t\tself.weight_vector = tf.reduce_sum(self.weight_encoder ** 2, axis = 1)\n",
    "\n",
    "\n",
    "class unbalanced_DAE(object):\n",
    "\n",
    "\t\"\"\"\n",
    "\tAn unbalanced version of DAE. the differences is that a weight pos_weight is added\n",
    "\tto the MSE reconstruction loss for positive examples. For this purpose, the labels\n",
    "\tof the examples are used.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, n_feature, n_hidden, noise_scale, noise_std,\n",
    "\t             posi_weight = 1.0, reg_lamda = 0.00):\n",
    "\n",
    "\t\tself.n_hidden = n_hidden\n",
    "\t\tself.n_feature = n_feature\n",
    "\t\tself.reg_lamda = reg_lamda\n",
    "\t\tself.noise_scale = noise_scale\n",
    "\t\tself.noise_std = noise_std\n",
    "\t\tself.posi_weight = posi_weight\n",
    "\n",
    "\t\tself.data = tf.placeholder(shape = [None, n_feature], dtype = tf.float64)\n",
    "\t\tself.label = tf.placeholder(shape = [None, 1], dtype = tf.float64)\n",
    "\t\tself.noise = self.noise_scale * tf.random_normal([n_feature], dtype = tf.float64,\n",
    "\t\t                                                  stddev = self.noise_std)\n",
    "\t\tdata_with_noise = self.data + self.noise\n",
    "\n",
    "\t\tself.weight_encoder = tf.get_variable(name = 'weight_encoder', \n",
    "\t\t\t                    shape = [self.n_feature, self.n_hidden],\n",
    "\t\t\t                    dtype = tf.float64)\n",
    "\t\tself.bias_encoder = tf.Variable(tf.zeros([self.n_hidden], \n",
    "\t\t\t                                      dtype = tf.float64),\n",
    "\t\t                                name = 'bias_encoder')\n",
    "\n",
    "\t\tweight_decoder = tf.get_variable(name = 'weight_decoder', \n",
    "\t\t\t                    shape = [self.n_hidden, self.n_feature],\n",
    "\t\t\t                    dtype = tf.float64)\n",
    "\t\tbias_decoder = tf.Variable(tf.zeros([self.n_feature], dtype = tf.float64),\n",
    "\t\t\t                           name = 'bias_decoder')\n",
    "\n",
    "\t\twith tf.name_scope('Encoder'):\n",
    "\t\t\tdata_encoded = tf.add(tf.matmul(data_with_noise, self.weight_encoder),\n",
    "\t\t\t\t                  self.bias_encoder)\n",
    "\t\t\tdata_encoded = tf.nn.sigmoid(data_encoded)\n",
    "\n",
    "\t\twith tf.name_scope('Decoder'):\n",
    "\t\t\tdata_recons = tf.add(tf.matmul(data_encoded, weight_decoder),\n",
    "\t\t\t\t                 bias_decoder)\n",
    "\t\t\tself.data_recons = tf.nn.sigmoid(data_recons)\n",
    "\n",
    "\t\twith tf.name_scope('Loss'):\n",
    "\t\t\tdiff = self.data_recons - self.data\n",
    "\t\t\tweights = self.label * (posi_weight -1) + 1\n",
    "\t\t\tweights = tf.reshape(weights, shape = [-1])\n",
    "\t\t\tself.loss_mse = 0.5 * tf.reduce_mean(tf.reduce_sum(diff**2, axis = 1) * weights)\n",
    "\t\t\tloss_reg = tf.reduce_sum(tf.sqrt(tf.reduce_sum(self.weight_encoder ** 2, axis = 1))) \n",
    "\t\t\tself.loss_reg = self.reg_lamda * loss_reg\n",
    "\t\t\tself.l2_loss = tf.nn.l2_loss(weight_decoder) * 1E-3\n",
    "\n",
    "\t\t\tself.loss = self.loss_mse + self.loss_reg + self.l2_loss\n",
    "\n",
    "\t\twith tf.name_scope('weight_vector'):\n",
    "\t\t\tself.weight_vector = tf.reduce_sum(self.weight_encoder ** 2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif __name__ == '__main__':\\n\\n    num_train = 125973\\n    num_test = 22543\\n    file_folder = 'normalized/'\\n   \\n    num_classes = 2\\n    trans_dataset(file_folder + 'train+.tfrecords', file_folder + 'train2.txt', num_train, num_classes)\\n    trans_dataset(file_folder + 'test+.tfrecords', file_folder + 'test2.txt', num_test, num_classes)\\n\\n    num_classes = 5\\n    trans_dataset(file_folder + 'train5.tfrecords', file_folder + 'train5.txt', num_train, num_classes)\\n    trans_dataset(file_folder + 'test5.tfrecords', file_folder + 'test5.txt', num_test, num_classes)\\n\\n    file_folder_new = file_folder + 'cross_validation_5/'\\n\\n    file_train = file_folder + 'train5.txt'\\n    file_test = file_folder +'test5.txt'\\n    split_dataset(file_train, file_test, k =10, file_folder_new = file_folder_new)\\n\\n    dataset = np.loadtxt(file_folder +'train_new.txt')\\n    #dataset = [file_folder + str(x + 1) + '_train.csv' for x in range(4)]\\n    posi, neg = parse_pos_neg(dataset)\\n\\n    np.savetxt(file_folder + 'train_posi.txt',posi )\\n    np.savetxt(file_folder + 'train_neg.txt', neg)\\n    \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def P_R_F1(confusion_matrix):\n",
    "\n",
    "    category = confusion_matrix.shape[0]\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for i in range (category):\n",
    "        TP = confusion_matrix[i,i]\n",
    "\n",
    "        precsion_temp = TP/np.sum(confusion_matrix[:,i])\n",
    "        recall_temp = TP/np.sum(confusion_matrix[i,:])\n",
    "        f1_temp = 2*precsion_temp*recall_temp/(precsion_temp + recall_temp)\n",
    "\n",
    "        precision.append(precsion_temp)\n",
    "        recall.append(recall_temp)\n",
    "        f1.append(f1_temp)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# shaping labels to one-hot vectors for trainning\n",
    "def label_coding(label, batch_size, category):\n",
    "    new_label = tf.cast(label, dtype = tf.int32)\n",
    "    new_label = tf.reshape(new_label, [batch_size, 1])\n",
    "    new_label = tf.one_hot(new_label, depth = category)\n",
    "    return tf.reshape(new_label, [batch_size, category])\n",
    "\n",
    "# get next batch of data and label\n",
    "def next_batch(filename, batch_size, conf, buffer_size = 0):\n",
    "\n",
    "    len_feature = conf.len_feature\n",
    "    len_label = conf.len_label\n",
    "    num_classes = conf.num_classes\n",
    "    one_hot_encoding = conf.one_hot_encoding\n",
    "\n",
    "    def read_data(examples):\n",
    "        features = {\"features\": tf.FixedLenFeature([len_feature], tf.float32),\n",
    "                    \"label_2\": tf.FixedLenFeature([len_label], tf.float32),\n",
    "                    \"label_10\": tf.FixedLenFeature([len_label], tf.float32)}\n",
    "        parsed_features = tf.parse_single_example(examples, features)\n",
    "        return parsed_features['features'], parsed_features['label_2'], \\\n",
    "               parsed_features['label_10']\n",
    "\n",
    "    data = tf.data.TFRecordDataset(filename)\n",
    "    data = data.map(read_data)\n",
    "    if buffer_size != 0:\n",
    "        data = data.shuffle(buffer_size = buffer_size)\n",
    "    data = data.repeat()\n",
    "    data = data.batch(batch_size)\n",
    "    iterator = data.make_one_shot_iterator()\n",
    "    next_data, next_label_2, next_label_10 = iterator.get_next()\n",
    "\n",
    "    if one_hot_encoding == True:\n",
    "        if num_classes == 2:\n",
    "            next_label_2 = label_coding(next_label_2, batch_size,\n",
    "                                        num_classes)\n",
    "        else:\n",
    "            next_label_10 = label_coding(next_label_10, batch_size,\n",
    "                                         num_classes)\n",
    "\n",
    "    return next_data, next_label_2, next_label_10\n",
    "\n",
    "\n",
    "def trans_dataset(file_tfr, file_txt, num_examples, num_classes):\n",
    "    with tf.Session() as sess:\n",
    "        all_data, all_label = next_batch(file_tfr, num_examples)\n",
    "        all_label = label_coding(all_label, num_examples, num_classes)\n",
    "\n",
    "        record = np.concatenate([sess.run(all_data), sess.run(all_label)], axis = 1)\n",
    "        np.savetxt(file_txt, record, fmt = '%.6e')\n",
    "\n",
    "def split_dataset(file_train, file_test, k, file_folder_new): # k is refer to k_fold\n",
    "\n",
    "    trainset = np.loadtxt(file_train)\n",
    "    testset = np.loadtxt(file_test)\n",
    "    dataset = np.concatenate((trainset, testset))\n",
    "\n",
    "    for i in range(k - 1):\n",
    "        trainset, testset = train_test_split(dataset, test_size = 1/(k - i))\n",
    "        dataset = trainset\n",
    "        np.savetxt(file_folder_new + str(i) + '.txt', testset)\n",
    "\n",
    "    np.savetxt(file_folder_new + str(k - 1) + '.txt', trainset)\n",
    "\n",
    "def get_dataset(file_folder, index_test, indices_train):\n",
    "\n",
    "    testset = np.loadtxt(file_folder + str(index_test) + '.txt')\n",
    "\n",
    "    count = 0\n",
    "    for other in indices_train:\n",
    "        temp = np.loadtxt(file_folder + str(other) + '.txt')\n",
    "\n",
    "        if count == 0:\n",
    "            trainset = temp\n",
    "        else:\n",
    "            trainset = np.concatenate((trainset, temp))\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "def parse_pos_neg(dataset):\n",
    "\n",
    "    label = dataset[:, -1]\n",
    "\n",
    "    record_posi = []\n",
    "    record_neg = []\n",
    "\n",
    "    records_len = dataset.shape[-1]\n",
    "    records_num = dataset.shape[0]\n",
    "\n",
    "    for index in range(records_num):\n",
    "        record = dataset[index, :]\n",
    "        record = np.reshape(record, (1, records_len))\n",
    "        if label[index] == 0.:\n",
    "            record_posi.append(record)\n",
    "        else:\n",
    "            record_neg.append(record)\n",
    "\n",
    "    posi = np.concatenate(record_posi)\n",
    "    neg = np.concatenate(record_neg)\n",
    "\n",
    "    return posi, neg\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    num_train = 125973\n",
    "    num_test = 22543\n",
    "    file_folder = 'normalized/'\n",
    "   \n",
    "    num_classes = 2\n",
    "    trans_dataset(file_folder + 'train+.tfrecords', file_folder + 'train2.txt', num_train, num_classes)\n",
    "    trans_dataset(file_folder + 'test+.tfrecords', file_folder + 'test2.txt', num_test, num_classes)\n",
    "\n",
    "    num_classes = 5\n",
    "    trans_dataset(file_folder + 'train5.tfrecords', file_folder + 'train5.txt', num_train, num_classes)\n",
    "    trans_dataset(file_folder + 'test5.tfrecords', file_folder + 'test5.txt', num_test, num_classes)\n",
    "\n",
    "    file_folder_new = file_folder + 'cross_validation_5/'\n",
    "\n",
    "    file_train = file_folder + 'train5.txt'\n",
    "    file_test = file_folder +'test5.txt'\n",
    "    split_dataset(file_train, file_test, k =10, file_folder_new = file_folder_new)\n",
    "\n",
    "    dataset = np.loadtxt(file_folder +'train_new.txt')\n",
    "    #dataset = [file_folder + str(x + 1) + '_train.csv' for x in range(4)]\n",
    "    posi, neg = parse_pos_neg(dataset)\n",
    "\n",
    "    np.savetxt(file_folder + 'train_posi.txt',posi )\n",
    "    np.savetxt(file_folder + 'train_neg.txt', neg)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0922 17:55:28.987432 139999097636672 deprecation.py:506] From /home/hll/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0922 17:55:29.085488 139999097636672 deprecation.py:323] From /home/hll/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0922 17:55:30.785737 139999097636672 deprecation.py:323] From <ipython-input-3-2107a0eb0a24>:53: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss on Train: 26.058424 mse on Train: 25.864874 reg on Train: 0.076035 Loss on Test: 22.511542 mse on Test: 22.234963 reg on Test: 0.089396\n",
      "36\n",
      "8\n",
      "Epoch: 2, Loss on Train: 21.639182 mse on Train: 21.301651 reg on Train: 0.099221 Loss on Test: 21.681457 mse on Test: 21.295057 reg on Test: 0.106385\n",
      "34\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from autoencoders import DAE, unbalanced_DAE\n",
    "#from utils import next_batch, P_R_F1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# system parameters\n",
    "\n",
    "class Configures(object):\n",
    "\n",
    "\tdef __init__(self):\n",
    "        # parameter of records\n",
    "\t\tself.len_feature = 202\n",
    "\t\tself.len_label = 1\n",
    "\t\t#self.num_classes = 2\n",
    "\t\tself.num_classes = 10\n",
    "\t\tself.one_hot_encoding = False\n",
    "\t\tself.num_records_train = 1625628\n",
    "\t\tself.num_records_test = 508012\n",
    "\n",
    "        # parameters for training\n",
    "\t\tself.batch_size = 256\n",
    "\t\tself.batch_size_test = 2048\n",
    "\t\tself.training_epochs = 2\n",
    "\t\tself.learn_rate_start = 1E-4\n",
    "\n",
    "\t\tself.batch_train = self.num_records_train//self.batch_size\n",
    "\t\tself.batch_test = self.num_records_test//self.batch_size_test\n",
    "\n",
    "n_hidden = 64\n",
    "noise_scale = 0.\n",
    "noise_std = 0.1\n",
    "conf = Configures()\n",
    "\n",
    "# training op\n",
    "with tf.Session() as sess:\n",
    "\n",
    "\t#AE = DAE(conf.len_feature, n_hidden, noise_scale, noise_std, reg_lamda = 0.001)\n",
    "\tAE = unbalanced_DAE(conf.len_feature, n_hidden, noise_scale, noise_std, posi_weight = 3.5, \n",
    "\t\t                reg_lamda = 0.001)\n",
    "\n",
    "\tglobal_step = tf.Variable(0, name = 'training_steps', trainable = False)\n",
    "\tlearn_rate = tf.train.exponential_decay(conf.learn_rate_start, global_step, 2000, 0.96, staircase=True)\n",
    "\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\twith tf.control_dependencies(update_ops):\n",
    "\t\toptimizer = tf.train.AdamOptimizer(conf.learn_rate_start)\n",
    "\t\tgrads_and_vars = optimizer.compute_gradients(AE.loss)\n",
    "\t\tgrads_clipped = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in grads_and_vars]\n",
    "\t\ttrain_op = optimizer.apply_gradients(grads_clipped, global_step = global_step)\n",
    "\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\n",
    "\t# Reading data\n",
    "\tfile_train = ['/home/hll/IDS/normalized/train.tfrecords', '/home/hll/IDS/normalized/validation.tfrecords']\n",
    "\tfile_test = '/home/hll/IDS/normalized/test.tfrecords'\n",
    "\ttrain_data, train_label, _ = next_batch(file_train, conf.batch_size, conf, 150)\n",
    "\ttest_data, test_label, _ = next_batch(file_test, conf.batch_size_test, conf)\n",
    "\n",
    "\tmin_loss = 100.\n",
    "\tfor epoch in range(conf.training_epochs):\n",
    "\t\ttime_start = time.time()\n",
    "\t\ttotal_mse_loss = 0.\n",
    "\t\ttotal_loss = 0.\n",
    "\t\ttotal_reg_loss = 0.\n",
    "\n",
    "\t\tfor step in range(conf.batch_train):\n",
    "\t\t\tdata, label = sess.run([train_data, train_label])\n",
    "\n",
    "\t\t\tfeed_dict = {AE.data: data, AE.label: label}\n",
    "\t\t\t_, loss, loss_mse, loss_reg = sess.run([train_op, AE.loss, AE.loss_mse, AE.loss_reg],\n",
    "\t\t\t                                        feed_dict = feed_dict)\n",
    "\n",
    "\t\t\ttotal_loss += loss\n",
    "\t\t\ttotal_mse_loss += loss_mse\n",
    "\t\t\ttotal_reg_loss += loss_reg\n",
    "\n",
    "\t\ttime_train_end = time.time()\n",
    "\t\ttest_loss = 0.\n",
    "\t\ttest_loss_reg = 0.\n",
    "\t\ttest_loss_mse = 0.\n",
    "\n",
    "\t\tfor step in range(conf.batch_test):\n",
    "\t\t\tdata, label = sess.run([test_data, test_label])\n",
    "\n",
    "\t\t\tfeed_dict = {AE.data: data, AE.label: label}\n",
    "\t\t\tweights, loss, loss_mse, loss_reg = sess.run([AE.weight_vector, AE.loss, AE.loss_mse, AE.loss_reg],\n",
    "\t\t\t                                              feed_dict = feed_dict)\n",
    "\t\t\ttest_loss += loss/conf.batch_test\n",
    "\t\t\ttest_loss_mse += loss_mse/conf.batch_test\n",
    "\t\t\ttest_loss_reg += loss_reg/conf.batch_test\n",
    "\n",
    "\t\ttime_test_end = time.time()\n",
    "\n",
    "\t\ttime_duration_train = int(time_train_end - time_start)\n",
    "\t\ttime_duration_test = int(time_test_end - time_train_end)\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\tif test_loss < min_loss:\n",
    "\t\t\tmin_loss = test_loss\n",
    "\t\t\tnp.savetxt('feature_select_AE/weights_new_3.5.txt', weights, fmt = \"%.6E\")\n",
    "\n",
    "\t\tprint(\"Epoch:\", \"%d,\" % (epoch + 1),\n",
    "\t\t\t  \"Loss on Train:\", \"{:.6f}\".format(total_loss/(conf.batch_train)),\n",
    "\t\t\t  \"mse on Train:\", \"{:.6f}\".format(total_mse_loss/conf.batch_train),\n",
    "\t\t\t  \"reg on Train:\", \"{:.6f}\".format(total_reg_loss/conf.batch_train),\n",
    "\t\t\t  \"Loss on Test:\", \"{:.6f}\".format(test_loss),\n",
    "\t\t\t  \"mse on Test:\", \"{:.6f}\".format(test_loss_mse),\n",
    "\t\t\t  \"reg on Test:\", \"{:.6f}\".format(test_loss_reg))\n",
    "\n",
    "\t\tprint(time_duration_train)\n",
    "\t\tprint(time_duration_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 13, 173, 8, 16, 175, 15, 7, 17, 3, 177, 30]\n",
      "dtcpb\n",
      "stcpb\n",
      "service_-\n",
      "dload\n",
      "dmeansz\n",
      "service_dns\n",
      "smeansz\n",
      "sload\n",
      "trans_depth\n",
      "sttl\n",
      "service_ftp-data\n",
      "ct_ftp\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pandas import read_csv\n",
    "\n",
    "def get_indices(num_select, num_feature, file_weights):\n",
    "\t\"\"\"\n",
    "\tThis function is to select maximum k features according to their\n",
    "\tweights.\n",
    "\n",
    "\tPram:\n",
    "\t\tnum_select: An interger, the number of the selected features\n",
    "\t\tnum_feature: An interger, the number of the original features\n",
    "\t\tfile_weights: A txt file storing a numpy array. Each row of the\n",
    "\t\t              array is the weight for a feature\n",
    "\tReturn:\n",
    "\t\ta list containing the indices of selected features\n",
    "\t\"\"\"\n",
    "\n",
    "\tx = np.arange(1, num_feature + 1)\n",
    "\ty = np.loadtxt(file_weights)\n",
    "\tindices = heapq.nlargest(num_select, range(len(y)), y.take)\n",
    "\tplt.scatter(x, y)\n",
    "\tplt.show()\n",
    "\tplt.savefig('weights_dis.eps', format = 'eps')\n",
    "\tprint(indices)\n",
    "\n",
    "\treturn indices\n",
    "\n",
    "\n",
    "def read_data(examples):\n",
    "    features = {\"features\": tf.FixedLenFeature([num_feature], tf.float32),\n",
    "                \"label_2\": tf.FixedLenFeature([len_label], tf.float32),\n",
    "                \"label_10\": tf.FixedLenFeature([len_label], tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(examples, features)\n",
    "    return parsed_features['features'], parsed_features['label_2'], \\\n",
    "           parsed_features['label_10']\n",
    "\n",
    "\n",
    "# get next batch of data and label\n",
    "def next_batch(filename, num_examples):\n",
    "\n",
    "    data = tf.data.TFRecordDataset(filename)\n",
    "    data = data.map(read_data)\n",
    "    data = data.batch(num_examples)\n",
    "    iterator = data.make_one_shot_iterator()\n",
    "    next_data, next_label_2, next_label_10 = iterator.get_next()\n",
    "    return next_data, next_label_2, next_label_10\n",
    "\n",
    "\n",
    "def make_tfrecords(dataset, file_to_save):\n",
    "\t[features, label_2, label_10] = dataset\n",
    "\n",
    "\twith tf.python_io.TFRecordWriter(file_to_save) as writer:\n",
    "\t\tfor index in range(features.shape[0]):\n",
    "\t\t\tfeature = {'features': tf.train.Feature(float_list = tf.train.FloatList(value = features[index, :])),\n",
    "\t\t\t           'label_2': tf.train.Feature(float_list = tf.train.FloatList(value = label_2[index, :])),\n",
    "\t\t\t           'label_10': tf.train.Feature(float_list = tf.train.FloatList(value = label_10[index, :]))}\n",
    "\t\t\texample = tf.train.Example(features = tf.train.Features(feature = feature))\n",
    "\t\t\twriter.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def selection(data, indices):\n",
    "\t\"\"\"\n",
    "\tselect the columns (indicating the features) according to the indices\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn data[:, indices]\n",
    "\n",
    "\n",
    "def select_feature(file, num_examples, indices):  \n",
    "\t\"\"\" \n",
    "\tThe main function of feature selection.\n",
    "\n",
    "\tParams:\n",
    "\t  file: The .tfrecords file containing original data.包含原始数据的.tfrecords文件\n",
    "\t  num_examples: The number of examples in the file  文件中的记录数\n",
    "\t  indices: The indices of features to be selected  被选择特征的索引\n",
    "\n",
    "\tReturn:\n",
    "\t  None\n",
    "\t  In the function, a new .tfrecords file with tail of 'selected'\n",
    "\t  will be created in the same folder with the original data \n",
    "\t\"\"\"\n",
    "\t\n",
    "\twith tf.Session() as sess:\n",
    "\t\tdata, label_2, label_10 = sess.run(next_batch(file, num_examples))\n",
    "\n",
    "\tdata_select = selection(data, indices)\n",
    "\n",
    "\tfile_name = file.split('\\\\')[-1] \n",
    "\tfile_tail = len('.tfrecords')\n",
    "\tfile_to_save = file_name[:-1*file_tail] + '_select_' + str(len(indices)) + '.tfrecords'\n",
    "\t\n",
    "\n",
    "\tmake_tfrecords([data_select, label_2, label_10], file_to_save)\n",
    "\n",
    "\n",
    "def show_feature_name(indices): \n",
    "\t\"\"\" \n",
    "\tThe function to convert indices to feature names\n",
    "\n",
    "\tParams:\n",
    "\t  indices:the indices of the features\n",
    "\tReturn:\n",
    "\t  None. \n",
    "\t  The name of features will be print\n",
    "\t\"\"\"\n",
    "\n",
    "\t#dirname = os.path.dirname(os.getcwd())\n",
    "\t#file = os.path.join(dirname, 'normalized/', '1_test.csv')\n",
    "\tfile = os.path.join( 'normalized/', '1_test.csv')\n",
    "\tdata = read_csv(file, index_col = 0)\n",
    "\tcols = data.columns\n",
    "\n",
    "\tfor x in indices:\n",
    "\t\tprint(cols[x])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tnum_select = 12   #选择出来的特征数。The number of selected features\n",
    "\tnum_feature = 202\n",
    "\tlen_label = 1\n",
    "\tfile_weights = 'feature_select_AE/weights_new_3.5.txt'\n",
    "\n",
    "\tindices = get_indices(num_select, num_feature, file_weights)\n",
    "\t#print(indices)\n",
    "\tshow_feature_name(indices)\n",
    "\n",
    "\tdirname = os.path.dirname(os.getcwd())\n",
    "\tfile_folder = os.path.join(dirname, '/home/hll/IDS/', 'normalized/')\n",
    "\t\n",
    "\tfile_train = file_folder + 'train.tfrecords'\n",
    "\tfile_valid = file_folder + 'validation.tfrecords'\n",
    "\tnum_examples_train = 1778030\n",
    "\tnum_examples_validation = 254005\n",
    "\n",
    "\tfile_test = file_folder + 'test.tfrecords'\n",
    "\tnum_examples_test = 508012\n",
    "\n",
    "\t\n",
    "\tselect_feature(file_train, num_examples_train, indices)\n",
    "\tselect_feature(file_valid, num_examples_validation, indices)\n",
    "\tselect_feature(file_test, num_examples_test, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 13, 173, 8, 16, 175, 15, 7, 17, 3, 177, 30]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14, 13, 173, 8, 16, 175, 15, 7, 17, 3, 177, 30]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_indices(num_select, num_feature, file_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
